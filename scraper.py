import requests
from bs4 import BeautifulSoup
import re
import sys
import time

# --- C·∫•u h√¨nh chung ---
BASE_URL = "https://govolunteerhcmc.vn"
FALLBACK_IMAGE_URL = "https://govolunteerhcmc.vn/wp-content/uploads/2024/02/logo-gv-tron.png"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
    'Referer': 'https://www.google.com/'
}

def get_high_res_image_url(url: str):
    """Lo·∫°i b·ªè c√°c h·∫≠u t·ªë k√≠ch th∆∞·ªõc ·∫£nh (-150x150, -300x200, v.v.) ƒë·ªÉ l·∫•y ·∫£nh g·ªëc ch·∫•t l∆∞·ª£ng cao."""
    if not url:
        return FALLBACK_IMAGE_URL
    return re.sub(r'-\d{2,4}x\d{2,4}(?=\.\w+$)', '', url)

def _scrape_generic_page(url: str, container_selector: str):
    """H√†m chung ƒë·ªÉ c√†o c√°c trang c√≥ c·∫•u tr√∫c section > h2 > article."""
    try:
        response = requests.get(url, headers=HEADERS, timeout=20)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"‚ùå L·ªói khi c√†o {url}: {e}", file=sys.stderr)
        return []

    soup = BeautifulSoup(response.text, "lxml")
    sections_data = []
    page_container = soup.select_one(container_selector)
    if not page_container:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y container '{container_selector}' t·∫°i {url}", file=sys.stderr)
        return []

    for sec in page_container.select("section.elementor-top-section"):
        h2 = sec.select_one("h2.elementor-heading-title")
        if not h2 or not h2.text.strip():
            continue
        category_name = h2.text.strip()
        
        articles = []
        for post in sec.select("article.elementor-post"):
            a_tag = post.select_one("h3.elementor-post__title a")
            if not a_tag or not a_tag.get('href'):
                continue

            img_tag = post.select_one(".elementor-post__thumbnail img")
            image_url = FALLBACK_IMAGE_URL
            if img_tag:
                src = img_tag.get('src') or img_tag.get('data-src')
                if src:
                    image_url = get_high_res_image_url(src)

            excerpt_tag = post.select_one(".elementor-post__excerpt p")
            excerpt = excerpt_tag.text.strip() if excerpt_tag else "Kh√¥ng c√≥ m√¥ t·∫£."
            
            articles.append({
                "title": a_tag.text.strip(),
                "link": a_tag['href'],
                "imageUrl": image_url,
                "excerpt": excerpt,
            })

        if articles:
            unique_articles = list({article['link']: article for article in articles}.values())
            sections_data.append({"category": category_name, "articles": unique_articles})
    
    return sections_data

# --- TRI·ªÇN KHAI C√ÅC H√ÄM SCRAPE ---

def scrape_news():
    """C√†o to√†n b·ªô b√†i vi·∫øt t·ª´ trang /news v√† c√°c trang con."""
    print("üöÄ B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu t·ª´ /news/...")
    all_articles = []
    page = 1
    max_pages = 1
    category_name = "Nh·∫≠t k√Ω t√¨nh nguy·ªán"
    base_news_url = f"{BASE_URL}/news/"

    while page <= max_pages:
        current_url = f"{base_news_url}{page}/" if page > 1 else base_news_url
        print(f"üìÑ ƒêang c√†o trang: {current_url}")
        try:
            response = requests.get(current_url, headers=HEADERS, timeout=20)
            response.raise_for_status()
        except requests.RequestException as e:
            print(f"‚ùå L·ªói khi c√†o trang {current_url}: {e}", file=sys.stderr)
            break

        soup = BeautifulSoup(response.text, "lxml")
        if page == 1:
            anchor = soup.select_one(".e-load-more-anchor[data-max-page]")
            if anchor: max_pages = int(anchor['data-max-page'])
            print(f"üîç T√¨m th·∫•y t·ªïng c·ªông {max_pages} trang.")

        container = soup.select_one(".elementor-1096")
        if not container:
            page += 1
            continue
            
        for post in container.select("article.elementor-post"):
            a_tag = post.select_one("h3.elementor-post__title a")
            if not a_tag or not a_tag.get('href'): continue
            img_tag = post.select_one(".elementor-post__thumbnail img")
            image_url = get_high_res_image_url(img_tag.get('src') if img_tag else None)
            excerpt_tag = post.select_one(".elementor-post__excerpt p")
            all_articles.append({
                "title": a_tag.text.strip(),
                "link": a_tag['href'],
                "imageUrl": image_url,
                "excerpt": excerpt_tag.text.strip() if excerpt_tag else "Kh√¥ng c√≥ m√¥ t·∫£.",
            })
        page += 1
        if page <= max_pages: time.sleep(1)

    unique_articles = list({article['link']: article for article in all_articles}.values())
    print(f"‚úÖ C√†o xong! T√¨m th·∫•y {len(unique_articles)} b√†i vi·∫øt ƒë·ªôc nh·∫•t.")
    return [{"category": category_name, "articles": unique_articles}] if unique_articles else []

def scrape_clubs():
    """C√†o d·ªØ li·ªáu c√°c CLB, ƒê·ªôi, Nh√≥m t·ª´ trang /clubs m·ªôt c√°ch ·ªïn ƒë·ªãnh."""
    url = f"{BASE_URL}/clubs/"
    print(f"üöÄ B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu t·ª´ {url}...")
    try:
        response = requests.get(url, headers=HEADERS, timeout=20)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"‚ùå L·ªói khi c√†o {url}: {e}", file=sys.stderr)
        return []

    soup = BeautifulSoup(response.text, "lxml")
    page_container = soup.select_one(".elementor-1048")
    if not page_container:
        return []

    category_map = {}
    current_category = None

    for section in page_container.select("section.elementor-top-section"):
        # N·∫øu section n√†y l√† m·ªôt ti√™u ƒë·ªÅ, ƒë·∫∑t n√≥ l√†m danh m·ª•c hi·ªán t·∫°i
        title_tag = section.select_one("h2.elementor-heading-title")
        if title_tag and title_tag.text.strip():
            current_category = title_tag.text.strip()
            if current_category not in category_map:
                category_map[current_category] = []
            continue

        # N·∫øu section n√†y ch·ª©a b√†i vi·∫øt v√† ƒë√£ c√≥ danh m·ª•c, th√™m b√†i vi·∫øt v√†o ƒë√≥
        if current_category:
            posts = section.select("article.ecs-post-loop, article.elementor-post")
            for post in posts:
                button = post.select_one("a.elementor-button")
                if not button or not button.get('href'): continue

                title = button.text.strip()
                link = button['href']
                img_tag = post.select_one(".elementor-widget-theme-post-featured-image img")
                image_url = get_high_res_image_url(img_tag.get('src') if img_tag else None)
                
                category_map[current_category].append({
                    "title": title, 
                    "link": link, 
                    "imageUrl": image_url, 
                    "excerpt": ""
                })

    # Chuy·ªÉn ƒë·ªïi map th√†nh list output cu·ªëi c√πng
    final_data = []
    for name, articles in category_map.items():
        if articles:
            unique_articles = list({article['link']: article for article in articles}.values())
            final_data.append({"category": name, "articles": unique_articles})

    print(f"‚úÖ C√†o xong /clubs! T√¨m th·∫•y {len(final_data)} danh m·ª•c.")
    return final_data


def scrape_chuong_trinh_chien_dich_du_an():
    """C√†o d·ªØ li·ªáu t·ª´ trang /chuong-trinh-chien-dich-du-an."""
    url = f"{BASE_URL}/chuong-trinh-chien-dich-du-an/"
    print(f"üöÄ B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu t·ª´ {url}...")
    data = _scrape_generic_page(url, ".elementor-1165")
    print(f"‚úÖ C√†o xong {url}! T√¨m th·∫•y {len(data)} danh m·ª•c.")
    return data

def scrape_skills():
    """C√†o d·ªØ li·ªáu t·ª´ trang /skills."""
    url = f"{BASE_URL}/skills/"
    print(f"üöÄ B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu t·ª´ {url}...")
    data = _scrape_generic_page(url, ".elementor-1181")
    print(f"‚úÖ C√†o xong {url}! T√¨m th·∫•y {len(data)} danh m·ª•c.")
    return data

def scrape_ideas():
    """C√†o d·ªØ li·ªáu t·ª´ trang /ideas."""
    url = f"{BASE_URL}/ideas/"
    print(f"üöÄ B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu t·ª´ {url}...")
    data = _scrape_generic_page(url, ".elementor-1242")
    print(f"‚úÖ C√†o xong {url}! T√¨m th·∫•y {len(data)} danh m·ª•c.")
    return data

def scrape_article_with_requests(article_url: str):
    """L·∫•y n·ªôi dung chi ti·∫øt c·ªßa m·ªôt b√†i vi·∫øt."""
    print(f"üöÄ S·ª≠ d·ª•ng `requests` ƒë·ªÉ l·∫•y d·ªØ li·ªáu b√†i vi·∫øt: {article_url}")
    try:
        response = requests.get(article_url, headers=HEADERS, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "lxml")
        content_div = soup.select_one(".elementor-widget-theme-post-content .elementor-widget-container")
        if not content_div:
            print("‚ùå Kh√¥ng t√¨m th·∫•y th·∫ª div ch·ª©a n·ªôi dung.", file=sys.stderr)
            return None
        print("‚úÖ L·∫•y n·ªôi dung b√†i vi·∫øt th√†nh c√¥ng!")
        return str(content_div)
    except requests.RequestException as e:
        print(f"‚ùå L·ªói khi d√πng requests cho b√†i vi·∫øt: {e}", file=sys.stderr)
        return None